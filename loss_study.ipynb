{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e-07\n",
      "1.1920929e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "from sklearn.metrics import jaccard_similarity_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import log_loss, brier_score_loss\n",
    "\n",
    "import tensorflow as tf\n",
    "# import keras.backend as K\n",
    "from keras import backend as K\n",
    "from keras import objectives\n",
    "from keras.losses import binary_crossentropy\n",
    "\n",
    "_EPSILON = K.epsilon()\n",
    "EPS = np.finfo(np.float32).eps\n",
    "print(_EPSILON)\n",
    "print(EPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$E = \\sum_{\\pmb{x}\\in\\Omega} w(\\pmb{x}) log(p_{l(\\pmb{x})}(\\pmb{x}))$$\n",
    "\n",
    "$$w(\\pmb{x}) = w_c(\\pmb{x}) + w_0 \\mathrm{exp}\\left(-\\frac{(d_1(\\pmb{x}) + d_2(\\pmb{x}))^2)}{2\\sigma^2}\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_border_weights(border_mask, w0=10, sigma=5):\n",
    "    g = np.exp(-)\n",
    "    return w0 * g\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Recall score\n",
    "2. IoU score\n",
    "3. Jaccard index\n",
    "4. F$_\\beta$ score\n",
    "5. Dice score\n",
    "6. Cos Dice Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perf_measure0(y_true, y_pred, axis=None):\n",
    "    yt = y_true.astype(bool)\n",
    "    yp = y_pred.astype(bool)\n",
    "    TP = np.sum(yt & yp, axis=axis)\n",
    "    TN = np.sum(~yt & ~yp, axis=axis)\n",
    "    FP = np.sum(~yt & yp, axis=axis)\n",
    "    FN = np.sum(yt & ~yp, axis=axis)\n",
    "    return TP, FP, TN, FN\n",
    "\n",
    "def perf_measure(y_true, y_pred, axis=None):\n",
    "    TP = np.sum(y_true * y_pred, axis=axis)\n",
    "    TN = np.sum((1 - y_true) * (1 - y_pred), axis=axis)\n",
    "    FP = np.sum((1 - y_true) * y_pred, axis=axis)\n",
    "    FN = np.sum(y_true * (1 - y_pred), axis=axis)\n",
    "    return TP, FP, TN, FN\n",
    "    \n",
    "def confusion_matrix_variants(TP, FP, TN, FN):\n",
    "    TPR = TP / (TP + FN)  # Sensitivity, hit rate, recall, or true positive rate\n",
    "    TNR = TN / (TN + FP)  # Specificity or true negative rate\n",
    "    PPV = TP / (TP + FP)  # Precision or positive predictive value\n",
    "    NPV = TN / (TN + FN)  # Negative predictive value\n",
    "    FPR = FP / (FP + TN)  # Fall out or false positive rate\n",
    "    FNR = FN / (TP + FN)  # False negative rate\n",
    "    FDR = FP / (TP + FP)  # False discovery rate\n",
    "    ACC = (TP + TN) / (TP + FP + FN + TN)  # Overall accuracy\n",
    "\n",
    "    #     print(f'{TP:>5}, {TN:>6}, {FP:>5}, {FN:>5}, {TPR:>.5f}, {TNR:>.5f}, {PPV:>.5f}, {NPV:>.5f}, {FPR:>.5f}, {FNR:>.5f}, {FDR:>.5f}, {ACC:>.5f}')\n",
    "    return TPR, TNR, PPV, NPV, FPR, FNR, FDR, ACC\n",
    "\n",
    "def recall(y_true, y_pred, axis=None, smooth=1e-3):\n",
    "    return (np.sum(y_true * y_pred, axis=axis) + smooth) / (np.sum(y_true, axis=axis) + smooth)\n",
    "\n",
    "def fbeta(y_true, y_pred, beta=2, axis=None, smooth=1e-3):\n",
    "    tp, fp, tn, fn = perf_measure(y_true, y_pred, axis=axis)\n",
    "    return ((beta**2 + 1) * tp + smooth) / ((beta**2 + 1) * tp + beta**2 * fn + fp + smooth)\n",
    "\n",
    "def iou(y_true, y_pred, axis=None):\n",
    "    i = np.sum((y_true * y_pred) > 0.5, axis=axis)\n",
    "    u = np.sum((y_true + y_pred) > 0.5, axis=axis) + EPS  # avoid division by zero\n",
    "    return i / u\n",
    "\n",
    "def dice_score(y_true, y_pred, axis=None, smooth=1e-3):\n",
    "    AB = np.sum(y_true * y_pred, axis=axis)\n",
    "    A = np.sum(y_true, axis=axis)\n",
    "    B = np.sum(y_pred, axis=axis)\n",
    "    return (2 * AB + smooth) / (A + B + smooth)\n",
    "\n",
    "def brier_loss(y_true, y_pred, **kwargs):\n",
    "    return np.sum((y_pred - y_true)**2, axis=-1)\n",
    "\n",
    "def print_headers(metrics):\n",
    "    res = [m.header for m in metrics]\n",
    "    fmts = ' '.join([m.hfmt for m in metrics])\n",
    "    print(fmts.format(*res))\n",
    "    \n",
    "def print_metrics(y_true, y_pred, metrics):\n",
    "    res = [m.func(y_true, y_pred, **m.kwargs) for m in metrics]\n",
    "    fmts = ' '.join([m.fmt for m in metrics])\n",
    "    print(fmts.format(*res))\n",
    "\n",
    "def print_losses(y_true, y_pred, losses):\n",
    "    res = [np.mean(m.func(y_true, y_pred, **m.kwargs)) for m in losses]\n",
    "    fmts = ' '.join([m.fmt for m in losses])\n",
    "    print(fmts.format(*res))\n",
    "\n",
    "def enlarge(arr1, arr2):\n",
    "    top0 = np.hstack([arr1, arr2])\n",
    "    bot0 = np.hstack([arr2, arr2])\n",
    "    return np.vstack([top0, bot0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bce_tensor(y_true, y_pred):\n",
    "    y_pred = K.clip(y_pred, _EPSILON, 1.0 - _EPSILON)\n",
    "    out = -(y_true * K.log(y_pred) + (1.0 - y_true) * K.log(1.0 - y_pred))\n",
    "    return K.mean(out, axis=-1)\n",
    "\n",
    "def bce_np(y_true, y_pred, **kwargs):\n",
    "    y_pred = np.clip(y_pred, _EPSILON, 1.0 - _EPSILON)\n",
    "    out = -(y_true * np.log(y_pred) + (1.0 - y_true) * np.log(1.0 - y_pred))\n",
    "    return np.mean(out, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_loss(_shape):\n",
    "    if _shape == '2d':\n",
    "        shape = (6, 7)\n",
    "    elif _shape == '3d':\n",
    "        shape = (5, 6, 7)\n",
    "    elif _shape == '4d':\n",
    "        shape = (8, 5, 6, 7)\n",
    "    elif _shape == '5d':\n",
    "        shape = (9, 8, 5, 6, 7)\n",
    "\n",
    "    y_a = np.random.random(shape)\n",
    "    y_b = np.random.random(shape)\n",
    "\n",
    "    out1 = K.eval(binary_crossentropy(K.variable(y_a), K.variable(y_b)))\n",
    "    out2 = K.eval(bce_tensor(K.variable(y_a), K.variable(y_b)))\n",
    "    out3 = bce_np(y_a, y_b)\n",
    "    \n",
    "    assert out1.shape == out2.shape\n",
    "    assert out1.shape == out3.shape\n",
    "    assert out1.shape == shape[:-1]\n",
    "    print(out1.shape)\n",
    "    print(np.linalg.norm(out1))\n",
    "    print(np.linalg.norm(out2))\n",
    "    print(np.linalg.norm(out3))\n",
    "    print(np.linalg.norm(out1-out2))\n",
    "\n",
    "def test_loss():\n",
    "    shape_list = ['2d', '3d', '4d', '5d']\n",
    "    for _shape in shape_list:\n",
    "        check_loss(_shape)\n",
    "        print('======================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can recover the bce by excluding the weighting factor alpha \n",
    "# and taking the mean of the focal_loss along the -1 axis.\n",
    "# Assuming that the last axis is the label axis as is the case in Tensorflow.\n",
    "def focal_loss_to_bce_wrapper(gamma=0, alpha=None, axis=-1):\n",
    "    def focal_loss_to_bce(y_true, y_pred):\n",
    "        y_pred_c = np.clip(y_pred, EPS, 1. - EPS)\n",
    "        pt_1 = np.where(np.equal(y_true, 1), y_pred_c, np.ones_like(y_pred))\n",
    "        pt_0 = np.where(np.equal(y_true, 0), y_pred_c, np.zeros_like(y_pred))\n",
    "        res1 = np.power(1. - pt_1, gamma) * np.log(     pt_1)\n",
    "        res0 = np.power(     pt_0, gamma) * np.log(1. - pt_0)\n",
    "        return -np.mean(res1 + res0, axis=axis)\n",
    "    return focal_loss_to_bce\n",
    "\n",
    "def focal_loss_wrapper_k1(gamma=2., alpha=.25, axis=-1):\n",
    "    def focal_loss_k1(y_true, y_pred):\n",
    "        y_pred_c = K.clip(y_pred, _EPSILON, 1.0 - _EPSILON)\n",
    "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred_c, tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred_c, tf.zeros_like(y_pred))\n",
    "        res1 =      alpha  * K.pow(1. - pt_1, gamma) * K.log(     pt_1)\n",
    "        res0 = (1 - alpha) * K.pow(     pt_0, gamma) * K.log(1. - pt_0)\n",
    "        return -K.mean(res1 + res0, axis=axis)\n",
    "    return focal_loss_k1\n",
    "\n",
    "def focal_loss_wrapper_n1(gamma=2., alpha=.25, axis=-1):\n",
    "    def focal_loss_n1(y_true, y_pred):\n",
    "        y_pred_c = np.clip(y_pred, EPS, 1. - EPS)\n",
    "        pt_1 = np.where(np.equal(y_true, 1), y_pred_c, np.ones_like(y_pred))\n",
    "        pt_0 = np.where(np.equal(y_true, 0), y_pred_c, np.zeros_like(y_pred))\n",
    "        res1 =      alpha  * np.power(1. - pt_1, gamma) * np.log(     pt_1)\n",
    "        res0 = (1 - alpha) * np.power(     pt_0, gamma) * np.log(1. - pt_0)\n",
    "        return -np.mean(res1 + res0, axis=axis)\n",
    "    return focal_loss_n1\n",
    "\n",
    "def focal_loss_wrapper_ns(gamma=2., alpha=.25, axis=-1):\n",
    "    def focal_loss_ns(y_true, y_pred):\n",
    "        y_pred_c = np.clip(y_pred, EPS, 1. - EPS)\n",
    "        pt_1 = np.where(np.equal(y_true, 1), y_pred_c, np.ones_like(y_pred))\n",
    "        pt_0 = np.where(np.equal(y_true, 0), y_pred_c, np.zeros_like(y_pred))\n",
    "        res1 =      alpha  * np.power(1. - pt_1, gamma) * np.log(     pt_1)\n",
    "        res0 = (1 - alpha) * np.power(     pt_0, gamma) * np.log(1. - pt_0)\n",
    "        loss = -np.mean(res1 + res0, axis=axis)\n",
    "        return np.sum(loss)\n",
    "    return focal_loss_ns\n",
    "\n",
    "def focal_loss_wrapper_k2(gamma=2., alpha=.25, axis=-1):\n",
    "    def focal_loss_k2(y_true, y_pred):\n",
    "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "        pt_1 = K.clip(pt_1, _EPSILON, 1.0 - _EPSILON)\n",
    "        pt_0 = K.clip(pt_0, _EPSILON, 1.0 - _EPSILON)\n",
    "        return -K.mean(alpha * K.pow(1.-pt_1, gamma) * K.log(pt_1) + (1-alpha) * K.pow(pt_0, gamma) * K.log(1.-pt_0), axis=axis)\n",
    "    return focal_loss_k2\n",
    "\n",
    "def focal_loss_wrapper_n2(gamma=2., alpha=.25, axis=-1):\n",
    "    def focal_loss_n2(y_true, y_pred):\n",
    "        pt_1 = np.where(np.equal(y_true, 1), y_pred, np.ones_like(y_pred))\n",
    "        pt_0 = np.where(np.equal(y_true, 0), y_pred, np.zeros_like(y_pred))\n",
    "        pt_1 = np.clip(pt_1, EPS, 1. - EPS)\n",
    "        pt_0 = np.clip(pt_0, EPS, 1. - EPS)\n",
    "        return -np.mean(alpha * np.power(1.-pt_1, gamma) * np.log(pt_1) + (1-alpha) * np.power(pt_0, gamma) * np.log(1.-pt_0), axis=axis)\n",
    "    return focal_loss_n2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_loss(_shape):\n",
    "    if _shape == '2d':\n",
    "        shape = (6, 7)\n",
    "    elif _shape == '3d':\n",
    "        shape = (5, 6, 7)\n",
    "    elif _shape == '4d':\n",
    "        shape = (8, 5, 6, 7)\n",
    "    elif _shape == '5d':\n",
    "        shape = (9, 8, 5, 6, 7)\n",
    "\n",
    "    y_true = np.random.randint(0, 2, (shape)) * 1.0\n",
    "    y_pred = np.random.random(shape)\n",
    "\n",
    "    focal_loss_fc = focal_loss_to_bce_wrapper(gamma=0, alpha=0)\n",
    "    focal_loss_k1 = focal_loss_wrapper_k1(gamma=0, alpha=0.5)\n",
    "    focal_loss_k2 = focal_loss_wrapper_k2(gamma=0, alpha=0.5)\n",
    "    focal_loss_n1 = focal_loss_wrapper_n1(gamma=0, alpha=0.5)\n",
    "    focal_loss_n2 = focal_loss_wrapper_n2(gamma=0, alpha=0.5)\n",
    "\n",
    "    out_ce = K.eval(binary_crossentropy(K.variable(y_true), K.variable(y_pred)))\n",
    "    out_fc = focal_loss_fc(y_true, y_pred)\n",
    "    out_k1 = K.eval(focal_loss_k1(K.variable(y_true), K.variable(y_pred)))\n",
    "    out_k2 = K.eval(focal_loss_k2(K.variable(y_true), K.variable(y_pred)))\n",
    "    out_n1 = focal_loss_n1(y_true, y_pred)\n",
    "    out_n2 = focal_loss_n2(y_true, y_pred)\n",
    "    \n",
    "    print('ce', np.sum(out_ce), out_ce.shape)\n",
    "    print('fc', np.sum(out_fc), out_fc.shape)\n",
    "    print('k1', np.linalg.norm(out_k1), out_k1.shape)\n",
    "    print('k2', np.linalg.norm(out_k2), out_k2.shape)\n",
    "    print('n1', np.linalg.norm(out_n1), out_n1.shape)\n",
    "    print('n2', np.linalg.norm(out_n2), out_n2.shape)\n",
    "    \n",
    "    assert out_k1.shape == out_ce.shape\n",
    "    assert out_k1.shape == out_fc.shape\n",
    "    assert out_k1.shape == out_k2.shape\n",
    "    assert out_k1.shape == out_n1.shape\n",
    "    assert out_k1.shape == out_n2.shape\n",
    "    assert out_k1.shape == shape[:-1]\n",
    "    print(np.linalg.norm(out_k2-out_n2))\n",
    "\n",
    "def test_loss():\n",
    "    shape_list = ['2d', '3d', '4d', '5d']\n",
    "    for _shape in shape_list:\n",
    "        check_loss(_shape)\n",
    "        print('======================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ce 6.094289 (6,)\n",
      "fc 6.094289466447662 (6,)\n",
      "k1 1.3171521 (6,)\n",
      "k2 1.3171524 (6,)\n",
      "n1 1.3171522854057975 (6,)\n",
      "n2 1.3171524232972118 (6,)\n",
      "1.2360156827758254e-07\n",
      "======================\n",
      "ce 28.42365 (5, 6)\n",
      "fc 28.423650374142237 (5, 6)\n",
      "k1 2.8164084 (5, 6)\n",
      "k2 2.8164086 (5, 6)\n",
      "n1 2.816408187125766 (5, 6)\n",
      "n2 2.816408487895681 (5, 6)\n",
      "2.167798827917469e-07\n",
      "======================\n",
      "ce 242.01654 (8, 5, 6)\n",
      "fc 242.01654257703984 (8, 5, 6)\n",
      "k1 8.441095 (8, 5, 6)\n",
      "k2 8.441096 (8, 5, 6)\n",
      "n1 8.441095835796608 (8, 5, 6)\n",
      "n2 8.441096690265704 (8, 5, 6)\n",
      "1.4951486450247199e-06\n",
      "======================\n",
      "ce 2150.2935 (9, 8, 5, 6)\n",
      "fc 2150.2934860677224 (9, 8, 5, 6)\n",
      "k1 24.600975 (9, 8, 5, 6)\n",
      "k2 24.600979 (9, 8, 5, 6)\n",
      "n1 24.60097522692255 (9, 8, 5, 6)\n",
      "n2 24.600977831849523 (9, 8, 5, 6)\n",
      "1.63639511444481e-05\n",
      "======================\n"
     ]
    }
   ],
   "source": [
    "test_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "Metric = namedtuple('Metric', 'header hfmt func fmt kwargs')\n",
    "\n",
    "metrics = [\n",
    "    Metric('recall', \"{:>8}\", recall, \"{:8.5f}\", {}), \n",
    "    Metric('f2', \"{:>8}\", fbeta, \"{:8.5f}\", {}), \n",
    "    Metric('iou', \"{:>8}\", iou, \"{:8.5f}\", {}), \n",
    "    Metric('dice', \"{:>8}\", dice_score, \"{:8.5f}\", {})]\n",
    "\n",
    "losses1 = [\n",
    "    Metric('brier2', \"{:>8}\", brier_score_loss, \"{:8.5f}\", {}),\n",
    "]\n",
    "losses3 = [\n",
    "#     Metric('bce', \"{:>8}\", bce_np, \"{:8.4f}\", {}),\n",
    "#     Metric('brier1', \"{:>8}\", brier_loss, \"{:8.4f}\", {}),\n",
    "#     Metric('fl_00_0', \"{:>8}\", focal_loss_wrapper_n1(gamma=0, alpha=0), \"{:8.4f}\", {}),\n",
    "    Metric('fl_00_1', \"{:>8}\", focal_loss_wrapper_ns(gamma=0, alpha=0.25), \"{:8.4f}\", {}),\n",
    "    Metric('fl_00_2', \"{:>8}\", focal_loss_wrapper_ns(gamma=0, alpha=0.5), \"{:8.4f}\", {}),\n",
    "    Metric('fl_00_3', \"{:>8}\", focal_loss_wrapper_ns(gamma=0, alpha=0.75), \"{:8.4f}\", {}),\n",
    "#     Metric('fl_00_4', \"{:>8}\", focal_loss_wrapper_n1(gamma=0, alpha=1), \"{:8.4f}\", {}),\n",
    "#     Metric('fl_05_0', \"{:>8}\", focal_loss_wrapper_n1(gamma=0.5, alpha=0), \"{:8.4f}\", {}),\n",
    "    Metric('fl_05_1', \"{:>8}\", focal_loss_wrapper_ns(gamma=0.5, alpha=0.25), \"{:8.4f}\", {}),\n",
    "    Metric('fl_05_2', \"{:>8}\", focal_loss_wrapper_ns(gamma=0.5, alpha=0.5), \"{:8.4f}\", {}),\n",
    "    Metric('fl_05_3', \"{:>8}\", focal_loss_wrapper_ns(gamma=0.5, alpha=0.75), \"{:8.4f}\", {}),\n",
    "#     Metric('fl_05_4', \"{:>8}\", focal_loss_wrapper_n1(gamma=0.5, alpha=1), \"{:8.4f}\", {}),\n",
    "#     Metric('fl_10_0', \"{:>8}\", focal_loss_wrapper_n1(gamma=1, alpha=0), \"{:8.4f}\", {}),\n",
    "    Metric('fl_10_1', \"{:>8}\", focal_loss_wrapper_ns(gamma=1, alpha=0.25), \"{:8.4f}\", {}),\n",
    "    Metric('fl_10_2', \"{:>8}\", focal_loss_wrapper_ns(gamma=1, alpha=0.5), \"{:8.4f}\", {}),\n",
    "    Metric('fl_10_3', \"{:>8}\", focal_loss_wrapper_ns(gamma=1, alpha=0.75), \"{:8.4f}\", {}),\n",
    "#     Metric('fl_10_4', \"{:>8}\", focal_loss_wrapper_n1(gamma=1, alpha=1), \"{:8.4f}\", {}),\n",
    "#     Metric('fl_20_0', \"{:>8}\", focal_loss_wrapper_ns(gamma=2, alpha=0), \"{:8.4f}\", {}),\n",
    "    Metric('fl_20_1', \"{:>8}\", focal_loss_wrapper_ns(gamma=2, alpha=0.25), \"{:8.4f}\", {}),\n",
    "    Metric('fl_20_2', \"{:>8}\", focal_loss_wrapper_ns(gamma=2, alpha=0.5), \"{:8.4f}\", {}),\n",
    "    Metric('fl_20_3', \"{:>8}\", focal_loss_wrapper_ns(gamma=2, alpha=0.75), \"{:8.4f}\", {}),\n",
    "#     Metric('fl_20_4', \"{:>8}\", focal_loss_wrapper_ns(gamma=2, alpha=1), \"{:8.4f}\", {}),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_none = np.array([\n",
    "    [0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0]]\n",
    ")\n",
    "\n",
    "y_sm1 = np.array([\n",
    "    [0,0,0,0,0,0],\n",
    "    [0,1,1,0,0,0],\n",
    "    [0,1,1,0,0,0],\n",
    "    [0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0]]\n",
    ")\n",
    "\n",
    "y_sm2 = np.array([\n",
    "    [0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0],\n",
    "    [0,0,0,1,1,0],\n",
    "    [0,0,0,1,1,0],\n",
    "    [0,0,0,0,0,0]]\n",
    ")\n",
    "\n",
    "y_big = np.array([\n",
    "    [0,0,0,0,0,0],\n",
    "    [0,1,1,1,1,0],\n",
    "    [0,1,1,1,1,0],\n",
    "    [0,1,1,1,1,0],\n",
    "    [0,1,1,1,1,0],\n",
    "    [0,0,0,0,0,0]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_none_2 = enlarge(y_none, y_none)\n",
    "y_sm1_2 = enlarge(y_sm1, y_none)\n",
    "y_sm2_2 = enlarge(y_sm2, y_none)\n",
    "y_big_2 = enlarge(y_big, y_none)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  recall       f2      iou     dice\n",
      " 1.00000  1.00000  0.00000  1.00000\n",
      " 1.00000  0.00006  0.00000  0.00006\n",
      " 0.99000  0.99198  1.00000  0.99498\n",
      " 0.00025  0.00005  0.00000  0.00013\n",
      " 0.99000  0.62187  0.25000  0.39922\n",
      " 0.24755  0.29136  0.25000  0.39682\n",
      " 0.00006  0.00002  0.00000  0.00006\n",
      "\n",
      "  recall       f2      iou     dice\n",
      " 1.00000  1.00000  0.00000  1.00000\n",
      " 1.00000  0.00006  0.00000  0.00006\n",
      " 0.99000  0.99198  1.00000  0.99498\n",
      " 0.00025  0.00005  0.00000  0.00013\n",
      " 0.99000  0.62187  0.25000  0.39922\n",
      " 0.24755  0.29136  0.25000  0.39682\n",
      " 0.00006  0.00002  0.00000  0.00006\n"
     ]
    }
   ],
   "source": [
    "scale_factor = 0.99 \n",
    "print_headers(metrics)\n",
    "print_metrics(y_none, y_none*scale_factor, metrics)\n",
    "print_metrics(y_none, y_big*scale_factor, metrics)\n",
    "print_metrics(y_big, y_big*scale_factor, metrics)\n",
    "print_metrics(y_sm1, y_sm2*scale_factor, metrics)\n",
    "print_metrics(y_sm1, y_big*scale_factor, metrics)\n",
    "print_metrics(y_big, y_sm2*scale_factor, metrics)\n",
    "print_metrics(y_big, y_none*scale_factor, metrics)\n",
    "print('')\n",
    "print_headers(metrics)\n",
    "print_metrics(y_none_2, y_none_2*scale_factor, metrics)\n",
    "print_metrics(y_none_2, y_big_2*scale_factor, metrics)\n",
    "print_metrics(y_big_2, y_big_2*scale_factor, metrics)\n",
    "print_metrics(y_sm1_2, y_sm2_2*scale_factor, metrics)\n",
    "print_metrics(y_sm1_2, y_big_2*scale_factor, metrics)\n",
    "print_metrics(y_big_2, y_sm2_2*scale_factor, metrics)\n",
    "print_metrics(y_big_2, y_none_2*scale_factor, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " fl_00_1  fl_00_2  fl_00_3  fl_05_1  fl_05_2  fl_05_3  fl_10_1  fl_10_2  fl_10_3  fl_20_1  fl_20_2  fl_20_3\n",
      "  0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000\n",
      "  0.0067   0.0134   0.0201   0.0007   0.0013   0.0020   0.0001   0.0001   0.0002   0.0000   0.0000   0.0000\n",
      "  6.9094   4.6085   2.3076   6.8733   4.5824   2.2915   6.8387   4.5592   2.2796   6.7703   4.5135   2.2568\n",
      "  9.2103   6.1402   3.0701   9.1642   6.1094   3.0547   9.1182   6.0788   3.0394   9.0271   6.0180   3.0090\n",
      "  4.9596   6.8492   8.7387   4.9481   6.8415   8.7349   4.9366   6.8338   8.7310   4.9138   6.8186   8.7234\n",
      "  7.9729  15.9457  23.9186   7.9714  15.9427  23.9141   7.9712  15.9424  23.9136   7.9712  15.9424  23.9136\n",
      " 10.6283  21.2565  31.8848  10.6283  21.2565  31.8848  10.6283  21.2565  31.8848  10.6283  21.2565  31.8848\n",
      "\n",
      " fl_00_1  fl_00_2  fl_00_3  fl_05_1  fl_05_2  fl_05_3  fl_10_1  fl_10_2  fl_10_3  fl_20_1  fl_20_2  fl_20_3\n",
      "  0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000\n",
      "  0.0034   0.0067   0.0101   0.0003   0.0007   0.0010   0.0000   0.0001   0.0001   0.0000   0.0000   0.0000\n",
      "  3.4547   2.3043   1.1538   3.4366   2.2912   1.1458   3.4193   2.2796   1.1398   3.3851   2.2568   1.1284\n",
      "  4.6052   3.0701   1.5351   4.5821   3.0547   1.5274   4.5591   3.0394   1.5197   4.5135   3.0090   1.5045\n",
      "  2.4798   3.4246   4.3694   2.4741   3.4207   4.3674   2.4683   3.4169   4.3655   2.4569   3.4093   4.3617\n",
      "  3.9864   7.9729  11.9593   3.9857   7.9714  11.9570   3.9856   7.9712  11.9568   3.9856   7.9712  11.9568\n",
      "  5.3141  10.6283  15.9424   5.3141  10.6283  15.9424   5.3141  10.6283  15.9424   5.3141  10.6283  15.9424\n"
     ]
    }
   ],
   "source": [
    "scale_factor = 0.99 \n",
    "losses = losses3\n",
    "print_headers(losses)\n",
    "print_losses(y_none, y_none*scale_factor, losses)\n",
    "print_losses(y_big, y_big*scale_factor, losses)\n",
    "print_losses(y_sm1, y_big*scale_factor, losses)\n",
    "print_losses(y_none, y_big*scale_factor, losses)\n",
    "print_losses(y_sm1, y_sm2*scale_factor, losses)\n",
    "print_losses(y_big, y_sm2*scale_factor, losses)\n",
    "print_losses(y_big, y_none*scale_factor, losses)\n",
    "print('')\n",
    "print_headers(losses)\n",
    "print_losses(y_none_2, y_none_2*scale_factor, losses)\n",
    "print_losses(y_big_2, y_big_2*scale_factor, losses)\n",
    "print_losses(y_sm1_2, y_big_2*scale_factor, losses)\n",
    "print_losses(y_none_2, y_big_2*scale_factor, losses)\n",
    "print_losses(y_sm1_2, y_sm2_2*scale_factor, losses)\n",
    "print_losses(y_big_2, y_sm2_2*scale_factor, losses)\n",
    "print_losses(y_big_2, y_none_2*scale_factor, losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " fl_00_1  fl_00_2  fl_00_3  fl_05_1  fl_05_2  fl_05_3  fl_10_1  fl_10_2  fl_10_3  fl_20_1  fl_20_2  fl_20_3\n",
      "  0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000\n",
      "  0.0402   0.0804   0.1206   0.0040   0.0080   0.0121   0.0004   0.0008   0.0012   0.0000   0.0000   0.0000\n",
      " 41.4566  27.6511  13.8457  41.2398  27.4945  13.7493  41.0322  27.3549  13.6777  40.6217  27.0812  13.5406\n",
      " 55.2620  36.8414  18.4207  54.9850  36.6567  18.3283  54.7094  36.4729  18.2365  54.1623  36.1082  18.0541\n",
      " 29.7579  41.0951  52.4323  29.6886  41.0489  52.4092  29.6197  41.0030  52.3863  29.4830  40.9118  52.3407\n",
      " 47.8372  95.6744 143.5116  47.8282  95.6563 143.4845  47.8273  95.6545 143.4818  47.8271  95.6543 143.4814\n",
      " 63.7695 127.5391 191.3086  63.7695 127.5391 191.3086  63.7695 127.5391 191.3086  63.7695 127.5391 191.3086\n",
      "\n",
      " fl_00_1  fl_00_2  fl_00_3  fl_05_1  fl_05_2  fl_05_3  fl_10_1  fl_10_2  fl_10_3  fl_20_1  fl_20_2  fl_20_3\n",
      "  0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000\n",
      "  0.0402   0.0804   0.1206   0.0040   0.0080   0.0121   0.0004   0.0008   0.0012   0.0000   0.0000   0.0000\n",
      " 41.4566  27.6511  13.8457  41.2398  27.4945  13.7493  41.0322  27.3549  13.6777  40.6217  27.0812  13.5406\n",
      " 55.2621  36.8414  18.4207  54.9850  36.6567  18.3283  54.7094  36.4729  18.2365  54.1623  36.1082  18.0541\n",
      " 29.7579  41.0951  52.4323  29.6886  41.0489  52.4092  29.6197  41.0030  52.3863  29.4830  40.9118  52.3407\n",
      " 47.8372  95.6744 143.5116  47.8282  95.6563 143.4845  47.8273  95.6545 143.4818  47.8271  95.6543 143.4814\n",
      " 63.7696 127.5391 191.3086  63.7695 127.5391 191.3086  63.7695 127.5391 191.3086  63.7695 127.5391 191.3086\n"
     ]
    }
   ],
   "source": [
    "scale_factor = 0.99 \n",
    "losses = losses3\n",
    "print_headers(losses)\n",
    "print_losses(y_none[..., None], y_none[..., None]*scale_factor, losses)\n",
    "print_losses(y_big[..., None], y_big[..., None]*scale_factor, losses)\n",
    "print_losses(y_sm1[..., None], y_big[..., None]*scale_factor, losses)\n",
    "print_losses(y_none[..., None], y_big[..., None]*scale_factor, losses)\n",
    "print_losses(y_sm1[..., None], y_sm2[..., None]*scale_factor, losses)\n",
    "print_losses(y_big[..., None], y_sm2[..., None]*scale_factor, losses)\n",
    "print_losses(y_big[..., None], y_none[..., None]*scale_factor, losses)\n",
    "print('')\n",
    "print_headers(losses)\n",
    "print_losses(y_none_2[..., None], y_none_2[..., None]*scale_factor, losses)\n",
    "print_losses(y_big_2[..., None], y_big_2[..., None]*scale_factor, losses)\n",
    "print_losses(y_sm1_2[..., None], y_big_2[..., None]*scale_factor, losses)\n",
    "print_losses(y_none_2[..., None], y_big_2[..., None]*scale_factor, losses)\n",
    "print_losses(y_sm1_2[..., None], y_sm2_2[..., None]*scale_factor, losses)\n",
    "print_losses(y_big_2[..., None], y_sm2_2[..., None]*scale_factor, losses)\n",
    "print_losses(y_big_2[..., None], y_none_2[..., None]*scale_factor, losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  brier2\n",
      " 0.00000\n",
      " 0.43560\n",
      " 0.00004\n",
      " 0.22001\n",
      " 0.32671\n",
      " 0.33334\n",
      " 0.44444\n",
      "\n",
      "  brier2\n",
      " 0.00000\n",
      " 0.10890\n",
      " 0.00001\n",
      " 0.05500\n",
      " 0.08168\n",
      " 0.08334\n",
      " 0.11111\n"
     ]
    }
   ],
   "source": [
    "scale_factor = 0.99 \n",
    "losses = losses1\n",
    "print_headers(losses)\n",
    "print_metrics(y_none.flatten(), y_none.flatten()*scale_factor, losses)\n",
    "print_metrics(y_none.flatten(), y_big.flatten()*scale_factor, losses)\n",
    "print_metrics(y_big.flatten(), y_big.flatten()*scale_factor, losses)\n",
    "print_metrics(y_sm1.flatten(), y_sm2.flatten()*scale_factor, losses)\n",
    "print_metrics(y_sm1.flatten(), y_big.flatten()*scale_factor, losses)\n",
    "print_metrics(y_big.flatten(), y_sm2.flatten()*scale_factor, losses)\n",
    "print_metrics(y_big.flatten(), y_none.flatten()*scale_factor, losses)\n",
    "print('')\n",
    "print_headers(losses)\n",
    "print_metrics(y_none_2.flatten(), y_none_2.flatten()*scale_factor, losses)\n",
    "print_metrics(y_none_2.flatten(), y_big_2.flatten()*scale_factor, losses)\n",
    "print_metrics(y_big_2.flatten(), y_big_2.flatten()*scale_factor, losses)\n",
    "print_metrics(y_sm1_2.flatten(), y_sm2_2.flatten()*scale_factor, losses)\n",
    "print_metrics(y_sm1_2.flatten(), y_big_2.flatten()*scale_factor, losses)\n",
    "print_metrics(y_big_2.flatten(), y_sm2_2.flatten()*scale_factor, losses)\n",
    "print_metrics(y_big_2.flatten(), y_none_2.flatten()*scale_factor, losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test soft dice loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     19,
     44
    ]
   },
   "outputs": [],
   "source": [
    "def soft_dice_coef_n0(y_true, y_pred, axis=-1, smooth=1e-3):\n",
    "    AB = np.sum(y_true * y_pred, axis=axis)\n",
    "    A = np.sum(y_true + y_pred, axis=axis)\n",
    "#     B = np.sum(y_pred, axis=axis)\n",
    "    return (2 * AB + smooth) / (A + smooth)\n",
    "\n",
    "def soft_dice_coef_k0(y_true, y_pred, axis=-1, smooth=1e-3):\n",
    "    AB = K.sum(y_true * y_pred, axis=axis)\n",
    "    A = K.sum(y_true, axis=axis)\n",
    "    B = K.sum(y_pred, axis=axis)\n",
    "    return (2. * AB + smooth) / (A + B + smooth)\n",
    "\n",
    "def soft_dice_loss_n0(y_true, y_pred, axis=-1, smooth=1e-3):\n",
    "    return 1 - soft_dice_coef_n0(y_true, y_pred, axis=axis, smooth=smooth)\n",
    "\n",
    "def soft_dice_loss_k0(y_true, y_pred, axis=-1, smooth=1e-3):\n",
    "    return 1 - soft_dice_coef_k0(y_true, y_pred, axis=axis, smooth=smooth)\n",
    "\n",
    "# https://www.jeremyjordan.me/semantic-segmentation/\n",
    "def soft_dice_loss_n2(y_true, y_pred, epsilon=1e-6):\n",
    "    \"\"\"\n",
    "    Soft dice loss calculation for arbitrary batch size, number of classes, and number of spatial dimensions.\n",
    "    Assumes the `channels_last` format.\n",
    "\n",
    "    # Arguments\n",
    "        y_true: b x X x Y( x Z...) x c One hot encoding of ground truth\n",
    "        y_pred: b x X x Y( x Z...) x c Network output, must sum to 1 over c channel (such as after softmax)\n",
    "        epsilon: Used for numerical stability to avoid divide by zero errors\n",
    "\n",
    "    # References\n",
    "        V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation\n",
    "        https://arxiv.org/abs/1606.04797\n",
    "        More details on Dice loss formulation\n",
    "        https://mediatum.ub.tum.de/doc/1395260/1395260.pdf (page 72)\n",
    "\n",
    "        Adapted from https://github.com/Lasagne/Recipes/issues/99#issuecomment-347775022\n",
    "    \"\"\"\n",
    "\n",
    "    # skip the batch and class axis for calculating Dice score\n",
    "    axes = tuple(range(1, len(y_pred.shape) - 1))\n",
    "    numerator = 2. * np.sum(y_pred * y_true, axes)\n",
    "    denominator = np.sum(np.square(y_pred) + np.square(y_true), axes)\n",
    "    return 1 - numerator / (denominator + epsilon)  # average over classes and batch\n",
    "\n",
    "def soft_dice_loss_k2(y_true, y_pred, epsilon=1e-6):\n",
    "    axes = tuple(range(1, len(y_pred.shape) - 1))\n",
    "    numerator = 2. * K.sum(y_pred * y_true, axes)\n",
    "    denominator = K.sum(K.square(y_pred) + K.square(y_true), axes)\n",
    "    return 1 - numerator / (denominator + epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def check_loss(_shape):\n",
    "    if _shape == '2d':\n",
    "        shape = (6, 7)\n",
    "    elif _shape == '3d':\n",
    "        shape = (5, 6, 7)\n",
    "    elif _shape == '4d':\n",
    "        shape = (8, 5, 6, 7)\n",
    "    elif _shape == '5d':\n",
    "        shape = (9, 8, 5, 6, 7)\n",
    "\n",
    "    y_true = np.random.randint(0, 2, (shape)) * 1.0\n",
    "    y_pred = np.random.random(shape)\n",
    "\n",
    "    out_n0 = soft_dice_loss_n0(y_true, y_pred)\n",
    "    out_k0 = K.eval(soft_dice_loss_k0(K.variable(y_true), K.variable(y_pred)))\n",
    "    \n",
    "    print('n0', out_n0.shape, np.mean(out_n0))\n",
    "    print('k0', out_k0.shape, K.eval(K.mean(K.variable(out_k0))))\n",
    "    \n",
    "    assert out_k0.shape == out_n0.shape\n",
    "    assert out_k0.shape == shape[:-1]\n",
    "    print(np.linalg.norm(out_k0-out_n0))\n",
    "\n",
    "def test_loss():\n",
    "    shape_list = ['2d', '3d', '4d', '5d']\n",
    "    for _shape in shape_list:\n",
    "        check_loss(_shape)\n",
    "        print('======================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n0 (6,) 0.5589039955482429\n",
      "k0 (6,) 0.558904\n",
      "8.952415159745165e-08\n",
      "======================\n",
      "n0 (5, 6) 0.447953447155229\n",
      "k0 (5, 6) 0.44795343\n",
      "2.1955494995942978e-07\n",
      "======================\n",
      "n0 (8, 5, 6) 0.5294329270967677\n",
      "k0 (8, 5, 6) 0.52943295\n",
      "5.972190077369066e-07\n",
      "======================\n",
      "n0 (9, 8, 5, 6) 0.525747611982823\n",
      "k0 (9, 8, 5, 6) 0.5257476\n",
      "1.6635018929073494e-06\n",
      "======================\n"
     ]
    }
   ],
   "source": [
    "test_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test mixing losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def bce_soft_dice_loss_n0(y_true, y_pred, frac=0.5):\n",
    "    return bce_np(y_true, y_pred) * frac + soft_dice_loss_n0(y_true, y_pred) * (1 - frac)\n",
    "\n",
    "def bce_soft_dice_loss_wrapper_k0(frac=0.5):\n",
    "    def bce_soft_dice_loss_k0(y_true, y_pred):\n",
    "        return binary_crossentropy(y_true, y_pred) * frac + soft_dice_loss_k0(y_true, y_pred) * (1 - frac)\n",
    "    return bce_soft_dice_loss_k0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "code_folding": [
     31
    ]
   },
   "outputs": [],
   "source": [
    "def check_loss(_shape):\n",
    "    if _shape == '2d':\n",
    "        shape = (6, 7)\n",
    "    elif _shape == '3d':\n",
    "        shape = (5, 6, 7)\n",
    "    elif _shape == '4d':\n",
    "        shape = (8, 5, 6, 7)\n",
    "    elif _shape == '5d':\n",
    "        shape = (9, 8, 5, 6, 7)\n",
    "\n",
    "    y_true = np.random.randint(0, 2, (shape)) * 1.0\n",
    "    y_pred = np.random.random(shape)\n",
    "\n",
    "    frac = 0.3\n",
    "    \n",
    "    bce_soft_dice_loss_k0 = bce_soft_dice_loss_wrapper_k0(frac=frac)\n",
    "    \n",
    "    bce_n1 = bce_np(y_true, y_pred)\n",
    "    dice_n1 = soft_dice_loss_n0(y_true, y_pred)\n",
    "    out_n1 = np.mean(bce_n1) * frac + np.mean(dice_n1) * (1 - frac)\n",
    "    print('bce__n1', bce_n1.shape, np.mean(bce_n1))\n",
    "    print('dice_n1', dice_n1.shape, np.mean(dice_n1))\n",
    "    print('out__n1', out_n1.shape, out_n1)\n",
    "\n",
    "    out_n0 = bce_soft_dice_loss_n0(y_true, y_pred, frac=frac)\n",
    "    print('out__n0', out_n0.shape, np.mean(out_n0))\n",
    "\n",
    "    bce_k1 = K.eval(binary_crossentropy(K.variable(y_true), K.variable(y_pred)))\n",
    "    dice_k1 = K.eval(soft_dice_loss_k0(K.variable(y_true), K.variable(y_pred)))\n",
    "    out_k1 = K.eval(K.mean(K.variable(bce_k1))) * frac + K.eval(K.mean(K.variable(dice_k1))) * (1 - frac)\n",
    "    print('bce__k1', bce_k1.shape, K.eval(K.mean(K.variable(bce_n1))))\n",
    "    print('dice_k1', dice_k1.shape, K.eval(K.mean(K.variable(dice_n1))))\n",
    "    print('out__k1', out_k1.shape, out_k1)\n",
    "\n",
    "    out_k0 = K.eval(bce_soft_dice_loss_k0(K.variable(y_true), K.variable(y_pred)))\n",
    "    print('out__k0', out_k0.shape, K.eval(K.mean(K.variable(out_k0))))\n",
    "    \n",
    "#     assert out_k1.shape == out_k0.shape\n",
    "#     assert out_k1.shape == out_k2.shape\n",
    "#     assert out_k1.shape == out_n0.shape\n",
    "#     assert out_k1.shape == out_n1.shape\n",
    "#     assert out_k1.shape == out_n2.shape\n",
    "    assert out_k0.shape == shape[:-1]\n",
    "    print(np.linalg.norm(out_k1-out_n1))\n",
    "\n",
    "def test_loss():\n",
    "    shape_list = ['2d', '3d', '4d', '5d']\n",
    "    for _shape in shape_list:\n",
    "        check_loss(_shape)\n",
    "        print('======================')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bce__n1 (6,) 1.0835870316421043\n",
      "dice_n1 (6,) 0.4906300013296098\n",
      "out__n1 () 0.6685171104233582\n",
      "out__n0 (6,) 0.6685171104233582\n",
      "bce__k1 (6,) 1.083587\n",
      "dice_k1 (6,) 0.49063\n",
      "out__k1 () 0.6685171157121659\n",
      "out__k0 (6,) 0.6685171\n",
      "5.288807725101208e-09\n",
      "======================\n",
      "bce__n1 (5, 6) 1.1214160428333548\n",
      "dice_n1 (5, 6) 0.5585305986973864\n",
      "out__n1 () 0.7273962319381768\n",
      "out__n0 (5, 6) 0.727396231938177\n",
      "bce__k1 (5, 6) 1.1214161\n",
      "dice_k1 (5, 6) 0.55853057\n",
      "out__k1 () 0.7273962676525116\n",
      "out__k0 (5, 6) 0.72739625\n",
      "3.571433482285613e-08\n",
      "======================\n",
      "bce__n1 (8, 5, 6) 1.0148600433635961\n",
      "dice_n1 (8, 5, 6) 0.5185417804015048\n",
      "out__n1 () 0.6674372592901321\n",
      "out__n0 (8, 5, 6) 0.6674372592901321\n",
      "bce__k1 (8, 5, 6) 1.01486\n",
      "dice_k1 (8, 5, 6) 0.5185418\n",
      "out__k1 () 0.6674319803714752\n",
      "out__k0 (8, 5, 6) 0.66743207\n",
      "5.278918656870246e-06\n",
      "======================\n",
      "bce__n1 (9, 8, 5, 6) 1.002251914616841\n",
      "dice_n1 (9, 8, 5, 6) 0.5241681726755963\n",
      "out__n1 () 0.6675932952579697\n",
      "out__n0 (9, 8, 5, 6) 0.6675932952579698\n",
      "bce__k1 (9, 8, 5, 6) 1.002252\n",
      "dice_k1 (9, 8, 5, 6) 0.5241682\n",
      "out__k1 () 0.6675933301448822\n",
      "out__k0 (9, 8, 5, 6) 0.6675933\n",
      "3.48869124700002e-08\n",
      "======================\n"
     ]
    }
   ],
   "source": [
    "test_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def focal_soft_dice_loss_wrapper(gamma=2., alpha=.25, frac=0.5):\n",
    "    focal_loss = focal_loss_wrapper(gamma=gamma, alpha=alpha)\n",
    "    def focal_soft_dice_loss(y_true, y_pred, frac=frac):\n",
    "        return focal_loss(y_true, y_pred) * frac + soft_dice_loss(y_true, y_pred) * (1 - frac)\n",
    "    return focal_soft_dice_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.10176404, 0.1008238 , 0.10009978, 0.10044594, 0.10178322])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_batch_true = np.random.choice(2, (5, 100, 100, 1))\n",
    "rand_batch_pred = np.random.choice(2, (5, 100, 100, 1))\n",
    "average_sample_loss(rand_batch_true, rand_batch_pred, fbeta, axis=(1, 2, 3)) / rand_batch_true.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_normalizer(old_norm_weights, index_array, new_batch_scores, alpha=None):\n",
    "    if alpha is None:\n",
    "        baseline = old_norm_weights * np.sum(new_batch_scores - old_norm_weights[index_array])\n",
    "    else:\n",
    "        baseline = old_norm_weights * alpha\n",
    "    \n",
    "    other_examples = np.ones((len(old_norm_weights),), dtype=int)\n",
    "    other_examples[index_array] = 0\n",
    "    index_other = np.where(other_examples)[0]\n",
    "    \n",
    "    baseline[index_array] = new_batch_scores\n",
    "    print(baseline, np.sum(baseline))\n",
    "    return baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2]\n"
     ]
    }
   ],
   "source": [
    "n_samples = 6\n",
    "n_subs = 3\n",
    "index_array = np.array([0, 1, 2])\n",
    "# index_array = np.random.choice(n_samples, n_subs, replace=False)\n",
    "print(index_array)\n",
    "other_examples = np.ones((n_samples,), dtype=int)\n",
    "other_examples[index_array] = 0\n",
    "index_other = np.where(other_examples)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.96342001 0.54880108 0.7793285  0.10600293 0.99062654 0.61411048]\n"
     ]
    }
   ],
   "source": [
    "print(old_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.24071722 0.13712178 0.19472067 0.02648557 0.24751496 0.15343979]\n",
      "[0.24071722 0.13712178 0.19472067]\n"
     ]
    }
   ],
   "source": [
    "old_weights = np.random.random((n_samples,))\n",
    "print(old_weights)\n",
    "old_norm_weights = old_weights / np.sum(old_weights)\n",
    "print(old_norm_weights)\n",
    "old_norm_sample = old_norm_weights[index_array]\n",
    "print(old_norm_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.24724735 0.37993896 0.31944128 0.09919289 0.86815457 0.61496554\n",
      " 0.72502535 0.7692111  0.93714747 0.59123661 0.082467   0.05772703]\n"
     ]
    }
   ],
   "source": [
    "batch_scores = np.random.random((12,))\n",
    "print(batch_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_scores *= 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.04343956 0.06675251 0.05612351 0.01742747 0.15252845 0.10804497\n",
      " 0.12738168 0.1351448  0.16465    0.10387597 0.01448885 0.01014222] 1.0\n"
     ]
    }
   ],
   "source": [
    "scaled_scores = batch_scores / np.sum(batch_scores)\n",
    "print(scaled_scores, np.sum(scaled_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.21550592 0.36639121 0.29759854 0.04715134 0.9215473  0.63364288\n",
      " 0.75879328 0.80903745 1.         0.60666042 0.02813213 0.        ] 5.684460484113896\n"
     ]
    }
   ],
   "source": [
    "scaled_scores = (batch_scores - np.min(batch_scores)) / (np.max(batch_scores) - np.min(batch_scores))\n",
    "print(scaled_scores, np.sum(scaled_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1267597  0.19478854 0.16377236 0.05085458 0.44508876 0.31528285\n",
      " 0.37170874 0.39436205 0.4804603  0.30311742 0.04227949 0.02959571] 2.918070515389947\n"
     ]
    }
   ],
   "source": [
    "norm_scores = batch_scores / np.linalg.norm(batch_scores)\n",
    "print(norm_scores, np.sum(norm_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_scores *= 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.13613562 0.20192301 0.13856829 0.14336397 0.15893467 0.22107444] 1.0000000000000002\n"
     ]
    }
   ],
   "source": [
    "scaled_scores = batch_scores / np.sum(batch_scores)\n",
    "print(scaled_scores, np.sum(scaled_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.7745268  0.0286402  0.08510061 0.26841729 1.        ] 2.156684898648847\n"
     ]
    }
   ],
   "source": [
    "scaled_scores = (batch_scores - np.min(batch_scores)) / (np.max(batch_scores) - np.min(batch_scores))\n",
    "print(scaled_scores, np.sum(scaled_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.32711837 0.48519796 0.33296378 0.34448726 0.38190187 0.53121665] 2.4028858817096452\n"
     ]
    }
   ],
   "source": [
    "norm_scores = batch_scores / np.linalg.norm(batch_scores)\n",
    "print(norm_scores, np.sum(norm_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (6,) (3,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-235-068a6ca5c85a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnew_batch_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mscaled_scores\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mold_norm_sample\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_batch_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_batch_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (6,) (3,) "
     ]
    }
   ],
   "source": [
    "new_batch_scores = (scaled_scores + old_norm_sample) / 2.0\n",
    "print(new_batch_scores, np.sum(new_batch_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.42688128 -0.05222697  0.32290874] 0.6975630530832811\n"
     ]
    }
   ],
   "source": [
    "# print(new_batch_scores - old_norm_sample)\n",
    "print(new_batch_scores - old_norm_sample, np.sum(new_batch_scores - old_norm_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.2312239964377052, 0.46633905664557596, -0.4958280743219811)"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1 = 1 - np.sum(new_batch_scores)\n",
    "s0 = np.sum(old_norm_weights[index_other])\n",
    "alpha = s1 / s0\n",
    "s1, s0, alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.57311872 0.05222697 0.60587831 0.03260416 0.06483034 0.07044757] 1.3991060568301126\n",
      "[0.57311872 0.05222697 0.60587831 0.03169849 0.06302949 0.06849069] 1.3944426662636566\n",
      "[ 0.57311872  0.05222697  0.60587831 -0.04490571 -0.08929083 -0.09702745] 0.9999999999999999\n",
      "[0.57311872 0.05222697 0.60587831 0.03079282 0.06122865 0.06653381] 1.389779275697201\n",
      "[0.57311872 0.05222697 0.60587831 0.02988714 0.05942781 0.06457694] 1.3851158851307452\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.57311872, 0.05222697, 0.60587831, 0.02988714, 0.05942781,\n",
       "       0.06457694])"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_normalizer(old_norm_weights, index_array, new_batch_scores, 0.36)\n",
    "get_normalizer(old_norm_weights, index_array, new_batch_scores, 0.35)\n",
    "get_normalizer(old_norm_weights, index_array, new_batch_scores, alpha)\n",
    "get_normalizer(old_norm_weights, index_array, new_batch_scores, 0.34)\n",
    "get_normalizer(old_norm_weights, index_array, new_batch_scores, 0.33)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2612719302317831"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(old_norm_weights[3:] - new_norm_weights[3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3941666666666667"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1.1825/3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
